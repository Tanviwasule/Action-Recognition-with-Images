# -*- coding: utf-8 -*-
"""action-recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14D7op31WJDLcnYM9r8ufj1klT6l2FWbF
"""

!pip install tensorflow

import tensorflow as tf
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import zipfile
import os

def unzip_folder(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)


zip_path = '/content/drive/MyDrive/Dataset (1).zip'
extract_to = '/content'
unzip_folder(zip_path,extract_to)

from tensorflow.keras.utils import image_dataset_from_directory

image_size = (500,500)
batch_size = 1
seed = 42
class_names = [
    "Interacting with computer",
    "Photographing",
    "Playing Instrument",
    "Riding Bike",
    "Riding Horse",
    "Running",
    "Walking"
]
dataset_path = "/content/Dataset"

print("Training set:")
train_set = image_dataset_from_directory(
    directory=dataset_path + "/TrainSet",
    labels="inferred",
    label_mode="categorical",
    #class_names=class_names,
    color_mode="rgb",
    batch_size=batch_size,
    image_size=image_size,
    crop_to_aspect_ratio=False,
    validation_split=0.12,
    subset="validation",
    seed=seed
)

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
i = 0
for images, labels in train_set.take(len(class_names)):
    lbl=labels.numpy()
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[0].numpy().astype("uint8"))
    lbl_idx = [i for i in range(len(lbl[0])) if lbl[0][i] != 0][0]
    plt.title(class_names[lbl_idx])
    plt.axis("off")
    i+=1

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten
from tensorflow.keras import Model
from tensorflow.keras.regularizers import l2
from time import time
from datetime import timedelta
import random

def createCNN(input_shape, nb_classes):
    nb_filters = 32
    filter_size = (3, 3)
    max_pooling_size = (2, 2)
    inner_dropout_ratio = 0.4
    output_dropout_ratio = 0.2
    kernel_regularizer = l2(0.01)
    kernel_initializer="he_normal"
    random.seed(seed)


    model = Sequential()

    model.add(Conv2D(nb_filters, filter_size, padding='same', activation='relu',
                     kernel_initializer=kernel_initializer,
                     input_shape=input_shape, name="Conv2D_1"))

    model.add(MaxPooling2D(pool_size=max_pooling_size, name="DownSampling_1"))
    model.add(Dropout(inner_dropout_ratio, name="Dropout_1"))


    model.add(Conv2D(nb_filters*2, filter_size, padding='same', activation='relu',
                     kernel_regularizer=kernel_regularizer, name="Conv2D_3"))

    model.add(MaxPooling2D(pool_size=max_pooling_size, name="DownSampling_2"))
    model.add(Dropout(inner_dropout_ratio, name="Dropout_2"))

    model.add(Conv2D(nb_filters*2, filter_size, padding='same', activation='relu',
                     kernel_regularizer=kernel_regularizer, name="Conv2D_5"))

    model.add(MaxPooling2D(pool_size=max_pooling_size, name="DownSampling_3"))
    model.add(Dropout(inner_dropout_ratio, name="Dropout_3"))


    flat_layer = Flatten()
    model.add(flat_layer)
    dense_size = model.output_shape[1]
    model.add(Dense(512, activation='relu', name="Dense_1"))

    model.add(Dropout(output_dropout_ratio, name="Dropout_4"))

    model.add(Dense(nb_classes, activation='softmax', name="Predictions"))

    return model

input_shape = train_set.element_spec[0].shape[1:]
nb_classes = train_set.element_spec[1].shape[1]
model = createCNN(input_shape, nb_classes)

feature_extractor = Model(
    inputs=model.inputs,

    outputs=model.get_layer(name="Conv2D_1").output,
)

X = train_set
y = []
features = None
for element in X:
    y.append(model(element[0]))
    features = feature_extractor(element[0])
    break

model.summary()

from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy

# Optimizer
optimizer = RMSprop()
# Loss fuction to minimize
loss = CategoricalCrossentropy()
# Metrics to monitor
metrics = [CategoricalAccuracy()]

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

import matplotlib.pyplot as plt
import numpy as np

# Accuracy curve
plt.figure(figsize=[8,6])
plt.plot(history.history['categorical_accuracy'],'r',linewidth=3.0)
plt.plot(history.history['val_categorical_accuracy'],'b',linewidth=3.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=16)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Accuracy',fontsize=16)
plt.title('Accuracy Curves',fontsize=16)

print(f"Mean accuracy of {np.mean(history.history['categorical_accuracy']):.3f}, mean validation accuracy of {np.mean(history.history['val_categorical_accuracy']):.3f}")

# Loss curve
plt.figure(figsize=[8,6])
plt.plot(history.history['loss'],'r',linewidth=3.0)
plt.plot(history.history['val_loss'],'b',linewidth=3.0)
plt.legend(['Training loss', 'Validation Loss'],fontsize=16)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.title('Loss Curves',fontsize=16)

print(f"Mean loss of {np.mean(history.history['loss']):.3f}, mean validation loss of {np.mean(history.history['val_loss']):.3f}")

time_start = time()
model.evaluate(test_set)
elapsed_time = time() - time_start
print(f"Testing time: {timedelta(seconds=elapsed_time)}.")

from tensorflow.math import confusion_matrix
from tensorflow import concat
from seaborn import heatmap

from tensorflow.keras.models import load_model
model = load_model('./models/model_350x350_50_32.h5')

def show_confusion_matrix(cm, labels):
  plt.figure(figsize=(10, 8))
  heatmap(cm, xticklabels=labels, yticklabels=labels,
              annot=True, fmt='g')
  plt.xlabel('Prediction')
  plt.ylabel('Actual')
  plt.show()

predictions = model.predict(test_set)
predicted_labels = argmax(predictions, axis=-1)
target_labels = concat([label for img, label in test_set], axis=0)
target_labels = argmax(target_labels, axis=-1) # 1D

conf_matrix = confusion_matrix(predictions=predicted_labels, labels=target_labels, num_classes=len(class_names))
show_confusion_matrix(conf_matrix, test_set.class_names)

from tensorflow.math import confusion_matrix
from tensorflow import concat
from seaborn import heatmap

from tensorflow.keras.models import load_model
model = load_model('./models/model_350x350_50_32.h5')

def show_confusion_matrix(cm, labels):
  plt.figure(figsize=(10, 8))
  heatmap(cm, xticklabels=labels, yticklabels=labels,
              annot=True, fmt='g')
  plt.xlabel('Prediction')
  plt.ylabel('Actual')
  plt.show()

predictions = model.predict(test_set)
predicted_labels = argmax(predictions, axis=-1)
target_labels = concat([label for img, label in test_set], axis=0)
target_labels = argmax(target_labels, axis=-1) # 1D

conf_matrix = confusion_matrix(predictions=predicted_labels, labels=target_labels, num_classes=len(class_names))
show_confusion_matrix(conf_matrix, test_set.class_names)

batch_size = 32 # 32, 64, 256
epochs = 50 # 50, 100
time_start = time()
history = model.fit(train_set, epochs=epochs,
                    batch_size=batch_size, verbose=1,
                    validation_data=val_set)
elapsed_time = time() - time_start
print(f"Training time: {timedelta(seconds=elapsed_time)}.")

model.save(
    filepath="./models/model_" + str(image_size[0]) + "x" + str(image_size[1]) + "_" +
    str(epochs) + "_" + str(batch_size) + ".h5",
    overwrite=False,
)